{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pickle\n",
    "import sys  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyung/Research23_Network_Analysis/mBIN/FTD_JupyterNotebook/Load_Dataset\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r loadData_hf\n",
    "sys.path.insert(0, loadData_hf)\n",
    "import findPathCoM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Directory Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the data folder\n",
    "%store -r dataDir\n",
    "\n",
    "# Directory path where Data will be saved to\n",
    "%store -r path_dataDir\n",
    "\n",
    "# Only used to load the FTDGeneralData_20221114.mat file --> Saved as NetworkDataGeneral\n",
    "%store -r baseDir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the preconstructed atlas data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the preconstructed Atlas data\n",
    "NetworkDataGeneral = scipy.io.loadmat(os.path.join(baseDir, 'NetworkAnalysisGeneral', 'FTDGeneralData_20221114.mat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] Loading Pathology Dataset - %AO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new_pathT: ex-vivo histopathology Data (Quantification) / %AO for pathology regions\n",
    "new_pathT = pd.read_excel(os.path.join(dataDir, 'NewFTDData', 'FTLD Library 4-25-23 update.xlsx'), \n",
    "                          dtype={'INDDID': str, 'Tau1_TDP2': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the Pathology Data - %AO to desired format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide each INDDID into {GM, WM} and {L, R} - 22 Regions (They are alphabetically Ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each INDDID divided into {GM, WM} and {L, R} (maximum 4 rows per INDDID)\n",
    "pathT_WMGM = pd.pivot_table(new_pathT, values='AvgPercentAO', \n",
    "                            index=['INDDID', 'FullAutopsyID', 'AutopsyIDNumOnly', \n",
    "                                   'Tau1_TDP2', 'Hemisphere_by_slide', 'AnalysisRegion'], \n",
    "                            columns=['Region'], aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstacking the Index --> Need a way to solve this without saving to csv format\n",
    "pathT_WMGM.to_csv(os.path.join(dataDir, 'NewFTDData', 'new_pathT(GMWM).csv'))\n",
    "pathT_WMGM = pd.read_csv(os.path.join(dataDir, 'NewFTDData', 'new_pathT(GMWM).csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide the pathT into GM and WM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathT_WMGM_type = pathT_WMGM.groupby('AnalysisRegion')\n",
    "\n",
    "# This contains 2 seperate rows for {L, R}\n",
    "pathT_GM_LR = pathT_WMGM_type.get_group('GM')\n",
    "pathT_WM_LR = pathT_WMGM_type.get_group('WM')\n",
    "\n",
    "# Combine 2 Rows for {L, R} into a single row\n",
    "pathT_GM_LR_type = pathT_GM_LR.groupby('Hemisphere_by_slide')\n",
    "pathT_GM_L = pathT_GM_LR_type.get_group('L')\n",
    "pathT_GM_R = pathT_GM_LR_type.get_group('R')\n",
    "pathT_GM = pd.merge(pathT_GM_L, pathT_GM_R, left_on=['INDDID', 'FullAutopsyID', 'AutopsyIDNumOnly', 'Tau1_TDP2', 'AnalysisRegion'], right_on=['INDDID', 'FullAutopsyID', 'AutopsyIDNumOnly', 'Tau1_TDP2', 'AnalysisRegion'], how='outer', suffixes=('_L', '_R')) \n",
    "\n",
    "pathT_WM_LR_type = pathT_WM_LR.groupby('Hemisphere_by_slide')\n",
    "pathT_WM_L = pathT_WM_LR_type.get_group('L')\n",
    "pathT_WM_R = pathT_WM_LR_type.get_group('R')\n",
    "pathT_WM = pd.merge(pathT_WM_L, pathT_WM_R, left_on=['INDDID', 'FullAutopsyID', 'AutopsyIDNumOnly', 'Tau1_TDP2', 'AnalysisRegion'], right_on=['INDDID', 'FullAutopsyID', 'AutopsyIDNumOnly', 'Tau1_TDP2', 'AnalysisRegion'], how='outer', suffixes=('_L', '_R'))\n",
    "\n",
    "# Drop Hemisphere_by_slide {L, R} Columns\n",
    "pathT_GM = pathT_GM.drop(columns=['Hemisphere_by_slide_L', 'Hemisphere_by_slide_R'])\n",
    "pathT_WM = pathT_WM.drop(columns=['Hemisphere_by_slide_L', 'Hemisphere_by_slide_R']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2] Mapping Pathology Regions to Atlas regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Look up table matching Atlas Region names to Atlas Labels(Index)\n",
    "pathLUT = pd.read_csv(os.path.join(dataDir,'schaefer_path_20210719_20220328.csv'))\n",
    "\n",
    "# Load the Look up table matching Pathology Region names to Atlas Region names\n",
    "AtlasToPathLUT = pd.read_excel(os.path.join(dataDir,'NewFTDData','PathToAtlasLUT_5_10_2023(mePFC_PFC_Ignored).xlsx'))\n",
    "\n",
    "# Using AtlasToPathLUT get the Pathology Regions and match them to Atlas Regions (Index 1~400 regions)\n",
    "# Return CoM for each Pathology Regions (Single Pahtology Regions match to multiple Atlas Regions, \n",
    "# therefore get Mean Value). Theses are unordered.\n",
    "# Also return list of Atlas regions index corrresponding to Pathology regions. Theses are unordered.\n",
    "pathCoMunordered, pathToAtlasIndexunordered = findPathCoM.findPathCoM(pathLUT, AtlasToPathLUT, \n",
    "                                                                      NetworkDataGeneral['NetworkDataGeneral'][0,0]['Schaefer400x7']['CoM'][0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get List of all regions of pathology we can map to 3D Atlas (out of 22) in Alphabetical Order\n",
    "# ['ANG', 'ATC', 'HIP', 'IFC', 'M1', 'MFC', 'OFC', 'PC', 'S1', 'SMTC', 'SPC', 'V1', 'aCING', 'aINS', 'aITC', 'dlPFC', 'iPFC', 'mPFC', 'pCING', 'pSTC']\n",
    "pathNames_3D_Map = np.sort(AtlasToPathLUT[\"PathSpreadSheetNames\"].values)\n",
    "\n",
    "# sn - denote the number of areas we are able to map to 3D Atlas\n",
    "sn = len(pathNames_3D_Map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordering the CoM so that it matches the order of Regions in the Pathology Dataset - %AO (Columns)\n",
    "pathCoM = np.empty((sn,3,2)) # One path regions corresponds to multiple atlas region\n",
    "pathToAtlasIndex = [[None, None] for _ in range(sn)]\n",
    "\n",
    "for s in range(sn):\n",
    "    idx = AtlasToPathLUT[AtlasToPathLUT.PathSpreadSheetNames == pathNames_3D_Map[s]].index[0] \n",
    "    pathCoM[s,:,:] = pathCoMunordered[idx, :, :]\n",
    "    pathToAtlasIndex[s] = pathToAtlasIndexunordered[idx]\n",
    "\n",
    "# pathCoM and pathToAtlasIndex are ordered by the order of pathNames_3D_Map (= Ordering of regions same as in PathT Dataset Columns Left to Right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Columns in pathT_GM / pathT_GM Where we cannot map to 3D Atlas, using AtlasToPathLUT (+5, for index offset)\n",
    "pathT_GM = pathT_GM.drop(pathT_GM.columns[[i + 5 for i, e in enumerate(pathT_GM.columns.values[5:]) if e.split(\"_\")[0] not in pathNames_3D_Map]], axis = 1)\n",
    "pathT_WM = pathT_WM.drop(pathT_WM.columns[[i + 5 for i, e in enumerate(pathT_WM.columns.values[5:]) if e.split(\"_\")[0] not in pathNames_3D_Map]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3] TAU and TDP Divide (GM) + Log %AO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get index of rows that are TAU and TDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index for the case with tau or tdp for patients\n",
    "FTD_TAUIndx = (pathT_GM.Tau1_TDP2 == 1)  # False or True\n",
    "FTD_TDPIndx = (pathT_GM.Tau1_TDP2 == 2) # False or True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Log %AO of Pathology Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Log %AO of 22 anatomical regions of the brain\n",
    "#pathData = np.ma.log(0.01 * pathT.iloc[:, 5:].values + 0.00015).filled(np.nan) # Masked log for handling the case where the value is NaN\n",
    "pathData = np.ma.log(pathT_GM.iloc[:, 5:].values + 0.00015).filled(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide Pathology Data into TAU and TDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log %AO of FTD TAU vs TDP --> Type: ndarray\n",
    "path_TAU = pathData[FTD_TAUIndx,:]\n",
    "path_TDP = pathData[FTD_TDPIndx,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique INNDID in whole dataset\n",
      "179\n",
      "Unique INDDID in GM\n",
      "179\n",
      "Unique INDDID in WM\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Unique INNDID in whole dataset\")\n",
    "print(len(pd.unique(pathT_WMGM['INDDID'])))\n",
    "print(\"Unique INDDID in GM\")\n",
    "print(len(pd.unique(pathT_GM_LR['INDDID'])))\n",
    "print(\"Unique INDDID in WM\")\n",
    "print(len(pd.unique(pathT_WM_LR['INDDID'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Dataset and Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save pathT GM/WM to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pathT GM/WM to csv\n",
    "pathT_GM.to_csv(os.path.join(path_dataDir, 'new_pathT(GM).csv'), index=False)\n",
    "pathT_WM.to_csv(os.path.join(path_dataDir, 'new_pathT(WM).csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_dataDir, 'sn.pkl'), 'wb') as f:\n",
    "    pickle.dump(sn, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save pathCoM, pathToAtlasIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_dataDir, 'pathCoM.pkl'), 'wb') as f:\n",
    "    pickle.dump(pathCoM, f)\n",
    "f.close()\n",
    "\n",
    "with open(os.path.join(path_dataDir, 'pathToAtlasIndex.pkl'), 'wb') as f:\n",
    "    pickle.dump(pathToAtlasIndex, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save TAU and TDP Pathology Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_TAU\n",
    "with open(os.path.join(path_dataDir, 'path_TAU.pkl'), 'wb') as f:\n",
    "    pickle.dump(path_TAU, f)\n",
    "f.close()\n",
    "\n",
    "# path_TDP\n",
    "with open(os.path.join(path_dataDir, 'path_TDP.pkl'), 'wb') as f:\n",
    "    pickle.dump(path_TDP, f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Network_Analysis",
   "language": "python",
   "name": "network_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
